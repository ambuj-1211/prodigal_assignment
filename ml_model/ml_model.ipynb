{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0023dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a0d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4.1-nano\",\n",
    "#             # model = \"gpt-4.1-nano-2025-04-14\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a search query generator that creates optimized search queries for finding job candidates.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             max_tokens=300,\n",
    "#             temperature=0.3\n",
    "#         )\n",
    "        \n",
    "#         search_query = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2d2ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>conversation</th>\n",
       "      <th>profanity</th>\n",
       "      <th>sensitive_data_compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742147aa-a2df-4429-859e-05b4a7875563</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, is this ...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b76917d8-cb80-481a-84d5-25de34e98f95</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Good afternoon,...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63803395-1ed2-405c-afdc-66ed0f48cd45</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a73d777f-368a-46b5-9d59-0aa5829b4427</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, is this ...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d7bbea61-d739-43fb-a198-ced1b59f9491</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...</td>\n",
       "      <td>Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0b6979e4-8c05-49e1-b7a7-94d85a627df5</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>928acb70-fa80-4532-a0c0-2234bf236e17</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...</td>\n",
       "      <td>Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b4dcda70-5083-4dc4-96a7-ce6d5e92c2a2</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2f279983-ea96-4c10-a09d-4b607de21a38</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hi, this is Mik...</td>\n",
       "      <td>Found</td>\n",
       "      <td>Violation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3f9a4a6a-2d53-4391-95c9-44306989d6ce</td>\n",
       "      <td>[{\"speaker\": \"Agent\", \"text\": \"Hello, is this ...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        conversation_id  \\\n",
       "0  742147aa-a2df-4429-859e-05b4a7875563   \n",
       "1  b76917d8-cb80-481a-84d5-25de34e98f95   \n",
       "2  63803395-1ed2-405c-afdc-66ed0f48cd45   \n",
       "3  a73d777f-368a-46b5-9d59-0aa5829b4427   \n",
       "4  d7bbea61-d739-43fb-a198-ced1b59f9491   \n",
       "5  0b6979e4-8c05-49e1-b7a7-94d85a627df5   \n",
       "6  928acb70-fa80-4532-a0c0-2234bf236e17   \n",
       "7  b4dcda70-5083-4dc4-96a7-ce6d5e92c2a2   \n",
       "8  2f279983-ea96-4c10-a09d-4b607de21a38   \n",
       "9  3f9a4a6a-2d53-4391-95c9-44306989d6ce   \n",
       "\n",
       "                                        conversation  profanity  \\\n",
       "0  [{\"speaker\": \"Agent\", \"text\": \"Hello, is this ...  Not Found   \n",
       "1  [{\"speaker\": \"Agent\", \"text\": \"Good afternoon,...  Not Found   \n",
       "2  [{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...  Not Found   \n",
       "3  [{\"speaker\": \"Agent\", \"text\": \"Hello, is this ...  Not Found   \n",
       "4  [{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...      Found   \n",
       "5  [{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...  Not Found   \n",
       "6  [{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...      Found   \n",
       "7  [{\"speaker\": \"Agent\", \"text\": \"Hello, this is ...  Not Found   \n",
       "8  [{\"speaker\": \"Agent\", \"text\": \"Hi, this is Mik...      Found   \n",
       "9  [{\"speaker\": \"Agent\", \"text\": \"Hello, is this ...  Not Found   \n",
       "\n",
       "  sensitive_data_compliance  \n",
       "0                 Not Found  \n",
       "1                 Not Found  \n",
       "2                 Not Found  \n",
       "3                 Not Found  \n",
       "4                 Not Found  \n",
       "5                 Not Found  \n",
       "6                 Not Found  \n",
       "7                 Not Found  \n",
       "8                 Violation  \n",
       "9                 Not Found  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"labeled_conversations.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2620e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"speaker\": \"Agent\", \"text\": \"Hello, is this Mark Johnson? This is Sarah calling from XYZ Collections. How are you today?\", \"stime\": 0, \"etime\": 8}, {\"speaker\": \"Customer\", \"text\": \"Hi Sarah, yes this is Mark. I\\'m doing okay, thanks.\", \"stime\": 8.5, \"etime\": 12}, {\"speaker\": \"Agent\", \"text\": \"I\\'m glad to hear that, Mark. I\\'m reaching out regarding your overdue balance with ABC Credit Union. Can I get your account number to look it up?\", \"stime\": 11, \"etime\": 21}, {\"speaker\": \"Customer\", \"text\": \"Sure, it\\'s 123456789.\", \"stime\": 22, \"etime\": 25}, {\"speaker\": \"Agent\", \"text\": \"Thank you for that. Now, your current balance is $500. Would you like to discuss payment options?\", \"stime\": 24, \"etime\": 34}, {\"speaker\": \"Customer\", \"text\": \"I was thinking of paying half today and the rest next month.\", \"stime\": 35, \"etime\": 42}, {\"speaker\": \"Agent\", \"text\": \"That sounds reasonable. I can help facilitate that. Just to clarify, can I confirm your mailing address?\", \"stime\": 41, \"etime\": 51}, {\"speaker\": \"Customer\", \"text\": \"It\\'s 456 Elm Street, Springfield.\", \"stime\": 52, \"etime\": 56}, {\"speaker\": \"Agent\", \"text\": \"Thank you, Mark! So, your first payment today would be $250, correct?\", \"stime\": 55, \"etime\": 64}, {\"speaker\": \"Customer\", \"text\": \"Yes, that\\'s right.\", \"stime\": 65, \"etime\": 67}, {\"speaker\": \"Agent\", \"text\": \"Great! Let\\'s go ahead and process that. How would you like to make the payment?\", \"stime\": 68, \"etime\": 76}, {\"speaker\": \"Customer\", \"text\": \"I can do it over the phone with my credit card.\", \"stime\": 77, \"etime\": 82}, {\"speaker\": \"Agent\", \"text\": \"Perfect. I just need the card number to get started.\", \"stime\": 81, \"etime\": 88}, {\"speaker\": \"Customer\", \"text\": \"It\\'s 5555 4444 3333 2222.\", \"stime\": 89, \"etime\": 92}, {\"speaker\": \"Agent\", \"text\": \"Thank you for providing that. Now, your payment will be processed shortly. Is there anything else I can assist you with today?\", \"stime\": 93, \"etime\": 103}]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"conversation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27f66da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"conversation\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e111b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa1b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 4 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   conversation_id            250 non-null    object\n",
      " 1   conversation               250 non-null    object\n",
      " 2   profanity                  250 non-null    object\n",
      " 3   sensitive_data_compliance  250 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 CALL ANALYSIS ML MODEL TRAINING 🔥\n",
      "============================================================\n",
      "=== TRAINING PHASE ===\n",
      "📊 Sample data created with 4 rows\n",
      "\n",
      "⚠️  TO TRAIN WITH YOUR REAL DATA:\n",
      "1. Load your CSV: df = pd.read_csv('your_data.csv')\n",
      "2. Uncomment: builder = CallAnalysisModelBuilder(df=df)\n",
      "3. Uncomment: builder.train_models()\n",
      "\n",
      "============================================================\n",
      "=== PREDICTION PHASE DEMO ===\n",
      "📝 Sample conversation loaded\n",
      "💡 After training, you can use:\n",
      "\n",
      "predictor = CallAnalysisPredictor()\n",
      "predictor.load_models()\n",
      "results = predictor.predict_both(sample_conversation)\n",
      "\n",
      "Expected output files after training:\n",
      "📦 profanity_model.pkl\n",
      "📦 sensitivedata_model.pkl\n",
      "📦 profanity_vectorizer.pkl\n",
      "📦 sensitive_vectorizer.pkl\n",
      "📦 label_encoder.pkl\n",
      "\n",
      "============================================================\n",
      "🚀 Ready to train your models!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CallAnalysisModelBuilder:\n",
    "    def __init__(self, df_path=None, df=None):\n",
    "        \"\"\"\n",
    "        Initialize the model builder\n",
    "        Args:\n",
    "            df_path: Path to CSV file containing the dataframe\n",
    "            df: Pandas dataframe (if already loaded)\n",
    "        \"\"\"\n",
    "        if df is not None:\n",
    "            self.df = df\n",
    "        elif df_path:\n",
    "            self.df = pd.read_csv(df_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either df_path or df must be provided\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.profanity_vectorizer = None\n",
    "        self.sensitive_vectorizer = None\n",
    "        self.profanity_model = None\n",
    "        self.sensitive_model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def preprocess_conversation_text(self, conversation_str):\n",
    "        \"\"\"\n",
    "        Extract and preprocess text from conversation JSON string\n",
    "        Args:\n",
    "            conversation_str: JSON string containing conversation data\n",
    "        Returns:\n",
    "            combined_text: Preprocessed text from all speakers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Parse JSON string\n",
    "            conversation_list = json.loads(conversation_str)\n",
    "            \n",
    "            # Extract all text and combine\n",
    "            all_text = []\n",
    "            for item in conversation_list:\n",
    "                if 'text' in item:\n",
    "                    all_text.append(item['text'])\n",
    "            \n",
    "            # Combine all text\n",
    "            combined_text = \" \".join(all_text)\n",
    "            \n",
    "            # Basic preprocessing\n",
    "            combined_text = combined_text.lower()\n",
    "            combined_text = re.sub(r'[^\\w\\s]', ' ', combined_text)  # Remove punctuation\n",
    "            combined_text = re.sub(r'\\s+', ' ', combined_text).strip()  # Remove extra spaces\n",
    "            \n",
    "            return combined_text\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error processing conversation: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_agent_customer_text(self, conversation_str):\n",
    "        \"\"\"\n",
    "        Extract separate agent and customer text for sensitive data analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conversation_list = json.loads(conversation_str)\n",
    "            agent_text = []\n",
    "            customer_text = []\n",
    "            \n",
    "            for item in conversation_list:\n",
    "                if 'text' in item and 'speaker' in item:\n",
    "                    text = item['text'].lower()\n",
    "                    if item['speaker'].lower() == 'agent':\n",
    "                        agent_text.append(text)\n",
    "                    else:\n",
    "                        customer_text.append(text)\n",
    "            \n",
    "            return \" \".join(agent_text), \" \".join(customer_text)\n",
    "        except:\n",
    "            return \"\", \"\"\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare data for training both models\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing conversation data...\")\n",
    "        \n",
    "        # Extract text from conversations\n",
    "        self.df['processed_text'] = self.df['conversation'].apply(self.preprocess_conversation_text)\n",
    "        \n",
    "        # Extract agent and customer text separately for sensitive data analysis\n",
    "        agent_customer_data = self.df['conversation'].apply(self.extract_agent_customer_text)\n",
    "        self.df['agent_text'] = [item[0] for item in agent_customer_data]\n",
    "        self.df['customer_text'] = [item[1] for item in agent_customer_data]\n",
    "        \n",
    "        # Combine agent and customer text for sensitive data analysis\n",
    "        self.df['combined_sensitive_text'] = self.df['agent_text'] + \" \" + self.df['customer_text']\n",
    "        \n",
    "        # Encode labels (assuming 'found'/'not found' format)\n",
    "        self.df['profanity_label'] = self.label_encoder.fit_transform(self.df['profanity'])\n",
    "        self.df['sensitive_label'] = self.label_encoder.fit_transform(self.df['sensitive_data_compliance'])\n",
    "        \n",
    "        print(f\"Data shape: {self.df.shape}\")\n",
    "        print(f\"Profanity distribution: {self.df['profanity'].value_counts()}\")\n",
    "        print(f\"Sensitive data distribution: {self.df['sensitive_data_compliance'].value_counts()}\")\n",
    "        \n",
    "    def create_profanity_keywords(self):\n",
    "        \"\"\"\n",
    "        Create profanity detection patterns and keywords\n",
    "        \"\"\"\n",
    "        profanity_patterns = [\n",
    "            r'\\b(damn|hell|shit|fuck|ass|bitch|crap)\\b',\n",
    "            r'\\b(stupid|idiot|moron|dumb)\\b',\n",
    "            r'\\b(wtf|omg|goddamn)\\b'\n",
    "        ]\n",
    "        return profanity_patterns\n",
    "    \n",
    "    def create_sensitive_data_keywords(self):\n",
    "        \"\"\"\n",
    "        Create sensitive data detection patterns\n",
    "        \"\"\"\n",
    "        sensitive_patterns = [\n",
    "            r'\\b\\d{3}-\\d{2}-\\d{4}\\b',  # SSN pattern\n",
    "            r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b',  # Credit card pattern\n",
    "            r'\\baccount\\s*number\\b',\n",
    "            r'\\bbalance\\b',\n",
    "            r'\\bssn\\b|social\\s*security',\n",
    "            r'\\bdate\\s*of\\s*birth\\b|dob\\b',\n",
    "            r'\\baddress\\b',\n",
    "            r'\\bverification\\b|verify\\b'\n",
    "        ]\n",
    "        return sensitive_patterns\n",
    "    \n",
    "    def build_profanity_model_sklearn(self):\n",
    "        \"\"\"\n",
    "        Build profanity detection model using scikit-learn\n",
    "        \"\"\"\n",
    "        print(\"Building profanity detection model (scikit-learn)...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X = self.df['processed_text']\n",
    "        y = self.df['profanity_label']\n",
    "        \n",
    "        # Create TF-IDF features\n",
    "        self.profanity_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        X_tfidf = self.profanity_vectorizer.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        self.profanity_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        self.profanity_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.profanity_model.predict(X_test)\n",
    "        print(\"Profanity Model Performance:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    def build_sensitive_data_model_tensorflow(self):\n",
    "        \"\"\"\n",
    "        Build sensitive data compliance model using TensorFlow\n",
    "        \"\"\"\n",
    "        print(\"Building sensitive data compliance model (TensorFlow)...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X = self.df['combined_sensitive_text']\n",
    "        y = self.df['sensitive_label']\n",
    "        \n",
    "        # Create TF-IDF features\n",
    "        self.sensitive_vectorizer = TfidfVectorizer(\n",
    "            max_features=1500,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        X_tfidf = self.sensitive_vectorizer.fit_transform(X).toarray()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Build neural network\n",
    "        model = Sequential([\n",
    "            Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Sensitive Data Model Performance:\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        self.sensitive_model = model\n",
    "        \n",
    "    def save_models(self):\n",
    "        \"\"\"\n",
    "        Save trained models and vectorizers\n",
    "        \"\"\"\n",
    "        print(\"Saving models...\")\n",
    "        \n",
    "        # Save profanity model (scikit-learn)\n",
    "        with open('profanity_model.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': self.profanity_model,\n",
    "                'vectorizer': self.profanity_vectorizer,\n",
    "                'label_encoder': self.label_encoder\n",
    "            }, f)\n",
    "        \n",
    "        # Save sensitive data model (TensorFlow)\n",
    "        self.sensitive_model.save('sensitive_model.h5')\n",
    "        \n",
    "        with open('sensitive_vectorizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.sensitive_vectorizer, f)\n",
    "        \n",
    "        print(\"Models saved successfully!\")\n",
    "        print(\"Files created:\")\n",
    "        print(\"- profanity_model.pkl\")\n",
    "        print(\"- sensitive_model.h5\")\n",
    "        print(\"- sensitive_vectorizer.pkl\")\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"\n",
    "        Complete training pipeline\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        self.build_profanity_model_sklearn()\n",
    "        self.build_sensitive_data_model_tensorflow()\n",
    "        self.save_models()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to use the classes\n",
    "    \n",
    "    print(\"🔥 CALL ANALYSIS ML MODEL TRAINING 🔥\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Training phase (run once)\n",
    "    print(\"=== TRAINING PHASE ===\")\n",
    "    \n",
    "    # Load your dataframe (replace with your actual data path)\n",
    "    try:\n",
    "        # Uncomment and modify this line to load your actual data:\n",
    "        # df = pd.read_csv('your_dataframe.csv')\n",
    "        \n",
    "        # For demonstration, create a sample dataframe structure\n",
    "        sample_data = {\n",
    "            'conversation_id': ['call_001', 'call_002', 'call_003', 'call_004'],\n",
    "            'conversation': [\n",
    "                '[{\"speaker\": \"Agent\", \"text\": \"Hello sir\", \"stime\": 0, \"etime\": 8}]',\n",
    "                '[{\"speaker\": \"Customer\", \"text\": \"This is damn frustrating\", \"stime\": 0, \"etime\": 5}]',\n",
    "                '[{\"speaker\": \"Agent\", \"text\": \"Your balance is $500, SSN 123-45-6789\", \"stime\": 0, \"etime\": 8}]',\n",
    "                '[{\"speaker\": \"Agent\", \"text\": \"Thank you for calling\", \"stime\": 0, \"etime\": 5}]'\n",
    "            ],\n",
    "            'profanity': ['not found', 'found', 'not found', 'not found'],\n",
    "            'sensitive_data_compliance': ['found', 'not found', 'found', 'not found']\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(f\"📊 Sample data created with {len(df)} rows\")\n",
    "        \n",
    "        # UNCOMMENT THESE LINES TO TRAIN WITH YOUR REAL DATA:\n",
    "        # builder = CallAnalysisModelBuilder(df=df)\n",
    "        # builder.train_models()\n",
    "        \n",
    "        print(\"\\n⚠️  TO TRAIN WITH YOUR REAL DATA:\")\n",
    "        print(\"1. Load your CSV: df = pd.read_csv('your_data.csv')\")\n",
    "        print(\"2. Uncomment: builder = CallAnalysisModelBuilder(df=df)\")\n",
    "        print(\"3. Uncomment: builder.train_models()\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in training phase: {e}\")\n",
    "    \n",
    "    # # 2. Prediction phase (use after training)\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"=== PREDICTION PHASE DEMO ===\")\n",
    "    \n",
    "    # # Example JSON input\n",
    "    # sample_conversation = '''[\n",
    "    #     {\"speaker\": \"Agent\", \"text\": \"Hello, is this Mark Johnson? This is Sarah calling from XYZ Collections. How are you today?\", \"stime\": 0, \"etime\": 8},\n",
    "    #     {\"speaker\": \"Customer\", \"text\": \"Hi Sarah, yes this is Mark. I'm doing okay, thanks.\", \"stime\": 8.5, \"etime\": 12},\n",
    "    #     {\"speaker\": \"Agent\", \"text\": \"Thank you for that. Now, your current balance is $500. Would you like to discuss payment options?\", \"stime\": 24, \"etime\": 34}\n",
    "    # ]'''\n",
    "    \n",
    "    # print(\"📝 Sample conversation loaded\")\n",
    "    # print(\"💡 After training, you can use:\")\n",
    "    # print()\n",
    "    # print(\"predictor = CallAnalysisPredictor()\")\n",
    "    # print(\"predictor.load_models()\")\n",
    "    # print(\"results = predictor.predict_both(sample_conversation)\")\n",
    "    # print()\n",
    "    print(\"Expected output files after training:\")\n",
    "    print(\"📦 profanity_model.pkl\")\n",
    "    print(\"📦 sensitivedata_model.pkl\") \n",
    "    print(\"📦 profanity_vectorizer.pkl\")\n",
    "    print(\"📦 sensitive_vectorizer.pkl\")\n",
    "    print(\"📦 label_encoder.pkl\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚀 Ready to train your models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947b0c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing conversation data...\n",
      "Data shape: (250, 10)\n",
      "Profanity distribution: profanity\n",
      "Not Found    211\n",
      "Found         39\n",
      "Name: count, dtype: int64\n",
      "Sensitive data distribution: sensitive_data_compliance\n",
      "Not Found    235\n",
      "Violation     10\n",
      "Found          5\n",
      "Name: count, dtype: int64\n",
      "Building profanity detection model (scikit-learn)...\n",
      "Profanity Model Performance:\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "Building sensitive data compliance model (TensorFlow)...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 21:05:24.714872: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.8750 - loss: 0.5786 - val_accuracy: 0.9500 - val_loss: 0.3883\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: 0.1845 - val_accuracy: 0.9500 - val_loss: 0.0455\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -0.1369 - val_accuracy: 0.9500 - val_loss: 0.0147\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -0.3368 - val_accuracy: 0.9500 - val_loss: 0.0225\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -0.6615 - val_accuracy: 0.9500 - val_loss: 0.0292\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -1.3161 - val_accuracy: 0.9500 - val_loss: 0.0270\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -2.4190 - val_accuracy: 0.9500 - val_loss: 0.0099\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -3.9020 - val_accuracy: 0.9500 - val_loss: 0.0023\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -6.1390 - val_accuracy: 0.9500 - val_loss: -0.0217\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -9.2682 - val_accuracy: 0.9500 - val_loss: -0.0964\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -14.5645 - val_accuracy: 0.9500 - val_loss: -0.2363\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -18.5687 - val_accuracy: 0.9500 - val_loss: -0.4014\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -28.1340 - val_accuracy: 0.9500 - val_loss: -0.5836\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -41.7969 - val_accuracy: 0.9500 - val_loss: -0.8507\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: -53.8073 - val_accuracy: 0.9500 - val_loss: -1.2131\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -80.9562 - val_accuracy: 0.9500 - val_loss: -1.5750\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -109.1606 - val_accuracy: 0.9500 - val_loss: -2.4716\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: -141.3240 - val_accuracy: 0.9500 - val_loss: -3.5394\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -181.3411 - val_accuracy: 0.9500 - val_loss: -4.8474\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9375 - loss: -236.1744 - val_accuracy: 0.9500 - val_loss: -6.1539\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -304.4365 - val_accuracy: 0.9500 - val_loss: -8.2483\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -391.0113 - val_accuracy: 0.9500 - val_loss: -10.3554\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: -496.7378 - val_accuracy: 0.9500 - val_loss: -13.5633\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -605.9050 - val_accuracy: 0.9500 - val_loss: -16.3209\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -810.8033 - val_accuracy: 0.9500 - val_loss: -20.1413\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -928.6808 - val_accuracy: 0.9500 - val_loss: -24.3903\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -1078.5656 - val_accuracy: 0.9500 - val_loss: -29.9209\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -1373.1711 - val_accuracy: 0.9500 - val_loss: -35.0704\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -1542.1129 - val_accuracy: 0.9500 - val_loss: -43.2019\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -1979.5354 - val_accuracy: 0.9500 - val_loss: -53.2049\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -2273.4377 - val_accuracy: 0.9500 - val_loss: -62.0498\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -2853.2673 - val_accuracy: 0.9500 - val_loss: -75.2701\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -3234.5237 - val_accuracy: 0.9500 - val_loss: -88.9355\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -3527.4602 - val_accuracy: 0.9500 - val_loss: -105.2668\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9438 - loss: -4217.6904 - val_accuracy: 0.9500 - val_loss: -121.5521\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: -4921.5137 - val_accuracy: 0.9500 - val_loss: -137.1391\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -5701.3096 - val_accuracy: 0.9500 - val_loss: -162.3180\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9438 - loss: -6444.6631 - val_accuracy: 0.9500 - val_loss: -182.0932\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -7197.3906 - val_accuracy: 0.9500 - val_loss: -208.4305\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9375 - loss: -9215.8242 - val_accuracy: 0.9500 - val_loss: -237.7332\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -9108.9229 - val_accuracy: 0.9500 - val_loss: -267.8105\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -10485.0986 - val_accuracy: 0.9500 - val_loss: -303.4191\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9438 - loss: -11692.5254 - val_accuracy: 0.9500 - val_loss: -349.1539\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9438 - loss: -13240.1582 - val_accuracy: 0.9500 - val_loss: -385.5906\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -14904.4688 - val_accuracy: 0.9500 - val_loss: -426.2828\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9438 - loss: -16966.5645 - val_accuracy: 0.9500 - val_loss: -477.7274\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -16978.2617 - val_accuracy: 0.9500 - val_loss: -534.5438\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9375 - loss: -19128.7461 - val_accuracy: 0.9500 - val_loss: -599.2250\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9375 - loss: -22993.5879 - val_accuracy: 0.9500 - val_loss: -654.4899\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9375 - loss: -23619.2383 - val_accuracy: 0.9500 - val_loss: -731.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive Data Model Performance:\n",
      "Test Accuracy: 0.9400\n",
      "Saving models...\n",
      "Models saved successfully!\n",
      "Files created:\n",
      "- profanity_model.pkl\n",
      "- sensitive_model.h5\n",
      "- sensitive_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"labeled_conversations.csv\")\n",
    "builder = CallAnalysisModelBuilder(df=df)\n",
    "builder.train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f164f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n"
     ]
    }
   ],
   "source": [
    "predictor = CallAnalysisPredictor()\n",
    "predictor.load_models()\n",
    "\n",
    "# Your JSON conversation string\n",
    "conversation_json = '[{\"speaker\":\"Agent\",\"text\":\"Hello, this is Mark calling from XYZ Collections. How are you today?\",\"stime\":0,\"etime\":7},{\"speaker\":\"Customer\",\"text\":\"I amm okay, just busy. What is this about?\",\"stime\":8,\"etime\":12},{\"speaker\":\"Agent\",\"text\":\"I understand, I will be brief. I\\'m calling regarding your outstanding balance with Quick Loans. Can you confirm your name for me?\",\"stime\":11,\"etime\":20},{\"speaker\":\"Customer\",\"text\":\"Sure, it\\'s Lisa Green.\",\"stime\":21,\"etime\":24},{\"speaker\":\"Agent\",\"text\":\"Thank you, Lisa. You currently have a balance of $450. How would you like to proceed with the payment?\",\"stime\":23,\"etime\":32},{\"speaker\":\"Customer\",\"text\":\"I need more information before I decide.\",\"stime\":33,\"etime\":36},{\"speaker\":\"Agent\",\"text\":\"Of course! Your last payment was made in January, and your balance has been accumulating since then. Would you like to set up a payment plan?\",\"stime\":35,\"etime\":45},{\"speaker\":\"Customer\",\"text\":\"I might consider that. What are the options?\",\"stime\":46,\"etime\":50},{\"speaker\":\"Agent\",\"text\":\"We can do weekly or monthly payments. For example, a monthly payment of $150 would work for you.\",\"stime\":49,\"etime\":58},{\"speaker\":\"Customer\",\"text\":\"That sounds reasonable, but I need to think it over.\",\"stime\":59,\"etime\":62},{\"speaker\":\"Agent\",\"text\":\"I understand. Please let me know if you have any questions. Can I count on you to respond by the end of the week?\",\"stime\":63,\"etime\":72},{\"speaker\":\"Customer\",\"text\":\"Yes, I\\'ll get back to you. Thank you.\",\"stime\":71,\"etime\":74},{\"speaker\":\"Agent\",\"text\":\"You\\'re welcome, Lisa. Have a great day!\",\"stime\":75,\"etime\":78},{\"speaker\":\"Customer\",\"text\":\"You too, bye.\",\"stime\":79,\"etime\":82}]'\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "profanity_result = predictor.predict_profanity(conversation_json)\n",
    "sensitive_result = predictor.predict_sensitive_data(conversation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "073fe862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Found'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profanity_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1dd457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'found'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8154de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"speaker\": \"Agent\", \"text\": \"Hello, this is Mark calling from XYZ Collections. How are you today?\", \"stime\": 0, \"etime\": 7}, {\"speaker\": \"Customer\", \"text\": \"I amm okay, just busy. What is this about?\", \"stime\": 8, \"etime\": 12}, {\"speaker\": \"Agent\", \"text\": \"I understand, I will be brief. I'm calling regarding your outstanding balance with Quick Loans. Can you confirm your name for me?\", \"stime\": 11, \"etime\": 20}, {\"speaker\": \"Customer\", \"text\": \"Sure, it's Lisa Green.\", \"stime\": 21, \"etime\": 24}, {\"speaker\": \"Agent\", \"text\": \"Thank you, Lisa. You currently have a balance of $450. How would you like to proceed with the payment?\", \"stime\": 23, \"etime\": 32}, {\"speaker\": \"Customer\", \"text\": \"I need more information before I decide.\", \"stime\": 33, \"etime\": 36}, {\"speaker\": \"Agent\", \"text\": \"Of course! Your last payment was made in January, and your balance has been accumulating since then. Would you like to set up a payment plan?\", \"stime\": 35, \"etime\": 45}, {\"speaker\": \"Customer\", \"text\": \"I might consider that. What are the options?\", \"stime\": 46, \"etime\": 50}, {\"speaker\": \"Agent\", \"text\": \"We can do weekly or monthly payments. For example, a monthly payment of $150 would work for you.\", \"stime\": 49, \"etime\": 58}, {\"speaker\": \"Customer\", \"text\": \"That sounds reasonable, but I need to think it over.\", \"stime\": 59, \"etime\": 62}, {\"speaker\": \"Agent\", \"text\": \"I understand. Please let me know if you have any questions. Can I count on you to respond by the end of the week?\", \"stime\": 63, \"etime\": 72}, {\"speaker\": \"Customer\", \"text\": \"Yes, I'll get back to you. Thank you.\", \"stime\": 71, \"etime\": 74}, {\"speaker\": \"Agent\", \"text\": \"You're welcome, Lisa. Have a great day!\", \"stime\": 75, \"etime\": 78}, {\"speaker\": \"Customer\", \"text\": \"You too, bye.\", \"stime\": 79, \"etime\": 82}]\n"
     ]
    }
   ],
   "source": [
    "data = [{\"speaker\":\"Agent\",\"text\":\"Hello, this is Mark calling from XYZ Collections. How are you today?\",\"stime\":0,\"etime\":7},{\"speaker\":\"Customer\",\"text\":\"I amm okay, just busy. What is this about?\",\"stime\":8,\"etime\":12},{\"speaker\":\"Agent\",\"text\":\"I understand, I will be brief. I\\'m calling regarding your outstanding balance with Quick Loans. Can you confirm your name for me?\",\"stime\":11,\"etime\":20},{\"speaker\":\"Customer\",\"text\":\"Sure, it\\'s Lisa Green.\",\"stime\":21,\"etime\":24},{\"speaker\":\"Agent\",\"text\":\"Thank you, Lisa. You currently have a balance of $450. How would you like to proceed with the payment?\",\"stime\":23,\"etime\":32},{\"speaker\":\"Customer\",\"text\":\"I need more information before I decide.\",\"stime\":33,\"etime\":36},{\"speaker\":\"Agent\",\"text\":\"Of course! Your last payment was made in January, and your balance has been accumulating since then. Would you like to set up a payment plan?\",\"stime\":35,\"etime\":45},{\"speaker\":\"Customer\",\"text\":\"I might consider that. What are the options?\",\"stime\":46,\"etime\":50},{\"speaker\":\"Agent\",\"text\":\"We can do weekly or monthly payments. For example, a monthly payment of $150 would work for you.\",\"stime\":49,\"etime\":58},{\"speaker\":\"Customer\",\"text\":\"That sounds reasonable, but I need to think it over.\",\"stime\":59,\"etime\":62},{\"speaker\":\"Agent\",\"text\":\"I understand. Please let me know if you have any questions. Can I count on you to respond by the end of the week?\",\"stime\":63,\"etime\":72},{\"speaker\":\"Customer\",\"text\":\"Yes, I\\'ll get back to you. Thank you.\",\"stime\":71,\"etime\":74},{\"speaker\":\"Agent\",\"text\":\"You\\'re welcome, Lisa. Have a great day!\",\"stime\":75,\"etime\":78},{\"speaker\":\"Customer\",\"text\":\"You too, bye.\",\"stime\":79,\"etime\":82}]\n",
    "\n",
    "conversation_str = json.dumps(data, ensure_ascii=False)\n",
    "\n",
    "print(conversation_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35d01ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor class and module\n",
    "\n",
    "class CallAnalysisPredictor:\n",
    "    \"\"\"\n",
    "    Class for making predictions on new data\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.profanity_components = None\n",
    "        self.sensitive_model = None\n",
    "        self.sensitive_vectorizer = None\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"\n",
    "        Load trained models\n",
    "        \"\"\"\n",
    "        # Load profanity model\n",
    "        with open('profanity_model.pkl', 'rb') as f:\n",
    "            self.profanity_components = pickle.load(f)\n",
    "        \n",
    "        # Load sensitive data model\n",
    "        self.sensitive_model = tf.keras.models.load_model('sensitive_model.h5')\n",
    "        \n",
    "        with open('sensitive_vectorizer.pkl', 'rb') as f:\n",
    "            self.sensitive_vectorizer = pickle.load(f)\n",
    "    \n",
    "    def preprocess_json_input(self, json_string):\n",
    "        \"\"\"\n",
    "        Preprocess JSON input for prediction\n",
    "        Args:\n",
    "            json_string: JSON string of conversation\n",
    "        Returns:\n",
    "            processed_text: Clean text for prediction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conversation_list = json.loads(json_string)\n",
    "            all_text = []\n",
    "            \n",
    "            for item in conversation_list:\n",
    "                if 'text' in item:\n",
    "                    all_text.append(item['text'])\n",
    "            \n",
    "            combined_text = \" \".join(all_text)\n",
    "            combined_text = combined_text.lower()\n",
    "            combined_text = re.sub(r'[^\\w\\s]', ' ', combined_text)\n",
    "            combined_text = re.sub(r'\\s+', ' ', combined_text).strip()\n",
    "            \n",
    "            return combined_text\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def predict_profanity(self, json_string):\n",
    "        \"\"\"\n",
    "        Predict profanity in conversation\n",
    "        \"\"\"\n",
    "        processed_text = self.preprocess_json_input(json_string)\n",
    "        \n",
    "        # Transform text\n",
    "        text_tfidf = self.profanity_components['vectorizer'].transform([processed_text])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.profanity_components['model'].predict(text_tfidf)[0]\n",
    "        \n",
    "        # Convert back to label\n",
    "        label_classes = self.profanity_components['label_encoder'].classes_\n",
    "        result = label_classes[prediction]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_sensitive_data(self, json_string):\n",
    "        \"\"\"\n",
    "        Predict sensitive data compliance violation\n",
    "        \"\"\"\n",
    "        processed_text = self.preprocess_json_input(json_string)\n",
    "        \n",
    "        # Transform text\n",
    "        text_tfidf = self.sensitive_vectorizer.transform([processed_text]).toarray()\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.sensitive_model.predict(text_tfidf)[0][0]\n",
    "        \n",
    "        # Convert to binary classification\n",
    "        result = \"found\" if prediction > 0.5 else \"not found\"\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011d8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodigal_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
